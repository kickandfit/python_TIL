{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "517604ba",
   "metadata": {},
   "source": [
    "# 색상 인지시키기\n",
    "- 모델링 : CNN, colab\n",
    "- 모델링 방법\n",
    "    - 목적 : 특정 사진이 들어 갔을 때, 사람 얼굴을 인지하고, 그 부분의 색중 대부분의 색을 뽑아내어, 매치시키기\n",
    "        - Mean-shift clustering이라는 이미지 세그먼테이션 기법 ( 대표색 뽑아내기 )( 이게 구현이 쉬워 ) ( 정 안될 때, 최후의 보루)\n",
    "        - Foreground Extraction 기법 사용( 주요 오프젝트 추출하기 )\n",
    "          ( GrabCut 알고리즘( 대상 물체 라인따는 알고리즘 )을 활용하여 Foreground Extraction을 수행 )\n",
    "        - 위의 두가지 알고리즘을 대표하는 딥러닝 기법 ( GRAD-CAM )\n",
    "        - Grad-CAM heatmap 정보와 guided backpropagation 정보를 혼합해서 GrabCut 알고리즘의 초기 mask로 사용하면 위에서 나열했던 여러 가지 문제점을 해결할 수 있음. 최종적으로 추출해낸 foreground 영역과 색상 정보\n",
    "        \n",
    "    - 사람 피부색에 해당하는 컬러셋 확보\n",
    "        - 선행되야할 고민\n",
    "        \n",
    "            - 무슨 모델로 학습시킬 것인가?\n",
    "            - 연속적인 데이터 처리는 어떻게 해야할까?\n",
    "        \n",
    "    - 이미지 인지는, CNN 을 통해 하고 GRID-CAM을 적용하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66a8d0",
   "metadata": {},
   "source": [
    "#### https://github.com/sjchoi86/deep-uncertainty/blob/master/code/demo_gradcam_resnet50.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72413e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "• 위의 이미지에서 시각화에 준 condition(Cat / Dog)에 따라 그에\n",
    "대응되는 부분만 표시해주는 것을 볼 수 있음\n",
    "• 이미지에서 heatmap을 계산하는 것이므로 앞에서 나온 Guided\n",
    "Backpropagation과 결합해 Guided-Grad-CAM으로도 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89b66d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-bed6db08992b>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-bed6db08992b>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    self.module = module\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "    class GradCAM:\n",
    "\n",
    "  def __init__(self, module, target_layer):\n",
    "    self.module = module\n",
    "    self.target_layer = target_layer\n",
    "    self.target_output = None\n",
    "    self.target_output_grad = None\n",
    "\n",
    "    def forward_hook(_, __, output):\n",
    "      self.target_output = output.clone()\n",
    "    \n",
    "    def backward_hook(_, __, grad_output):\n",
    "      assert len(grad_output) == 1\n",
    "      self.target_output_grad = grad_output[0].clone()\n",
    "\n",
    "    self.target_layer.register_forward_hook(forward_hook)\n",
    "    self.target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    def forward_pass(self, image):\n",
    "      self.module.eval()\n",
    "      self.module.zero_grad()\n",
    "      return self.module(image)\n",
    "\n",
    "    def get_grad_cam(self, image, target_class = None):\n",
    "      assert len(image.size()) == 3\n",
    "      image = image.unsqueeze(0)\n",
    "      out = self.forward_pass(image)\n",
    "      if target_class is None:\n",
    "        target_class = torch.argmax(out).item()\n",
    "        print('Target class is {}({})'.format(target_class, utils.index_to_class(target_class)))\n",
    "\n",
    "      onehot = utils.onehot(target_class, out_size(1))\n",
    "      onehot = onehot.unsqueeze(0)\n",
    "      out.backward(onehot)\n",
    "\n",
    "      grad = self.target_output_grad\n",
    "      grad = F.adaptive_avg_pool2d(grad, 1)\n",
    "\n",
    "      feature = self.target_output\n",
    "      feature = feature*grad\n",
    "      feature = torch.sum(feature, dim =1)\n",
    "      feature = F.relu(feature)\n",
    "\n",
    "      return feature.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7401d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160ab395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663a12e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d91c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4499f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
